{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T02:21:10.065293Z",
     "start_time": "2025-02-18T02:21:10.021165Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "from sacrebleu.metrics import BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T02:21:11.429860Z",
     "start_time": "2025-02-18T02:21:11.414537Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_llava_prediction_file(file_path):\n",
    "    \"\"\"\n",
    "    Read LLaVA's inference results (e.g., merge.jsonl) and extract data of different benchmarks based on 'category' field.\n",
    "    \"\"\"\n",
    "    predict_results = []\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm.tqdm(lines):\n",
    "            item = json.loads(line.strip())\n",
    "            predict_results.append(item)\n",
    "    print(\"Predicted Sample Number:\",len(predict_results))\n",
    "    benchmark_name_to_predicted_item_list = defaultdict(list)\n",
    "    for item in predict_results:\n",
    "        item_id = item['question_id']\n",
    "        category = item['category'] # {dataset_name}_for_{task_name}, e.g., TabFact_for_TFV\n",
    "        dataset_name = category.split('_for_')[0] # e.g., TabFact\n",
    "        task_name = category.split('_for_')[1] # e.g., TFV\n",
    "        # for table structure understanding tasks, benchmark name is the task name\n",
    "        if task_name not in ['TSD','TCL','RCE','MCD','TCE','TR','OOD_TSD','OOD_TCL','OOD_RCE','OOD_TCE']:\n",
    "            benchmark_name = dataset_name\n",
    "        else:\n",
    "            benchmark_name = task_name\n",
    "        benchmark_name_to_predicted_item_list[benchmark_name].append(item)\n",
    "    for benchmark_name,  predicted_item_list in benchmark_name_to_predicted_item_list.items():\n",
    "        item_num = len(predicted_item_list)\n",
    "        print(f'benchmark name: {benchmark_name}, test data num: {item_num}')\n",
    "    return benchmark_name_to_predicted_item_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Read Predicted Data and Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the ground truth data\n",
    "MMTab_eval_test_data = json.load(open(\"eval_data.json\"))\n",
    "# item_id --> test data\n",
    "item_id_to_test_item = {}\n",
    "for item in MMTab_eval_test_data:\n",
    "    item_id = item['item_id']\n",
    "    item_id_to_test_item[item_id] = item\n",
    "print(\"MMTab-eval data num: \",len(MMTab_eval_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TQA, TFV and T2T Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T02:21:20.095692Z",
     "start_time": "2025-02-18T02:21:20.061969Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tqa_answer_list(model_output):\n",
    "    \"\"\"\n",
    "    Extract the answer list from the model output to compute accuracy\n",
    "    \"\"\"\n",
    "    model_output = model_output.replace('\\n',' ')\n",
    "    ret = re.match('.*({[\\\"\\']answer[\\\"\\']\\:.*}).*',model_output)\n",
    "    if ret is not None:\n",
    "        answer_str = ret.group(1)\n",
    "        try:\n",
    "            answer_str = re.sub('[\\\"\\']+',\"\\\"\",answer_str)\n",
    "            answer_item = eval(answer_str)\n",
    "            predicted_answer = answer_item['answer']\n",
    "            if type(predicted_answer) != list and type(predicted_answer) == str:\n",
    "                predicted_answer = [predicted_answer]\n",
    "            elif type(predicted_answer) != list and type(predicted_answer) in [float,int]:\n",
    "                predicted_answer = [str(predicted_answer)]\n",
    "            else:\n",
    "                pass\n",
    "        # The answer is considered to be wrong if we can not extract answer list from the json str\n",
    "        except:\n",
    "            predicted_answer = []\n",
    "        return predicted_answer\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def evaluate_tqa_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table question answering (TQA) and table fact verification (TFV) benchmark.\n",
    "    Metric: accuracy.\n",
    "    Note that some baseline models can not strictly follow instructions to output the final answer in the required JSON format.\n",
    "    For instance, Qwen-VL may only output a short answer due to the potential overfitting of training data.\n",
    "    In such cases, the evaluation script needs to be changed according to the characteristic of certain model output.\n",
    "    \"\"\"\n",
    "    correct_item_list = []\n",
    "    wrong_item_list = []\n",
    "    failed_item_list = []\n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            model_output = item['text']\n",
    "            # parse the predicted answer list\n",
    "            predicted_answer_list = extract_tqa_answer_list(model_output)\n",
    "            gold_answer_list = ori_item['answer_list']\n",
    "            # Sometimes the order of multiple answer text is not necessarily same as the gold answer,\n",
    "            # so we convert the answer list to a set for comparison\n",
    "            if set(gold_answer_list) == set(predicted_answer_list):\n",
    "                correct_item_list.append(item)\n",
    "        except Exception:\n",
    "            failed_item_list.append(item)\n",
    "            \n",
    "    print(\"Benchmark: \",benchmark_name)\n",
    "    correct_num = len(correct_item_list)\n",
    "    total_sample_num = len(pred_item_list)\n",
    "    print(\"Accuracy: \", correct_num/total_sample_num)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "def evaluate_tqa_questions_hitab(benchmark_name, pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table question answering (TQA) and table fact verification (TFV) benchmark.\n",
    "    Metric: accuracy.\n",
    "    Note that some baseline models can not strictly follow instructions to output the final answer in the required JSON format.\n",
    "    For instance, Qwen-VL may only output a short answer due to the potential overfitting of training data.\n",
    "    In such cases, the evaluation script needs to be changed according to the characteristic of certain model output.\n",
    "    \"\"\"\n",
    "    correct_item_list = []\n",
    "    wrong_item_list = []\n",
    "    failed_item_list = []\n",
    "\n",
    "    def parse_answer(answer):\n",
    "        \"\"\"\n",
    "        Parse the answer using eval and return a number if it is a number, otherwise return the original string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            evaluated = eval(answer)  # Try to evaluate the answer\n",
    "            if isinstance(evaluated, (int, float)):  # Check if the result is a number\n",
    "                return float(evaluated)\n",
    "            return answer  # Return the original string if not a number\n",
    "        except (NameError, SyntaxError, TypeError):\n",
    "            return answer  # Return the original string if eval fails\n",
    "\n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            model_output = item['text']\n",
    "            # Parse the predicted answer list\n",
    "            predicted_answer_list = extract_tqa_answer_list(model_output)\n",
    "            gold_answer_list = ori_item['answer_list']\n",
    "\n",
    "            # Parse answers using the new logic\n",
    "            predicted_answer_list = [parse_answer(ans) for ans in predicted_answer_list]\n",
    "            gold_answer_list = [parse_answer(ans) for ans in gold_answer_list]\n",
    "\n",
    "            # Compare the sets of answers\n",
    "            if set(gold_answer_list) == set(predicted_answer_list):\n",
    "                correct_item_list.append(item)\n",
    "            else:\n",
    "                wrong_item_list.append(item)\n",
    "                # print(predicted_answer_list, item_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating item {item.get('question_id', 'unknown')}: {e}\")\n",
    "            failed_item_list.append(item)\n",
    "\n",
    "    print(\"Benchmark: \", benchmark_name)\n",
    "    correct_num = len(correct_item_list)\n",
    "    total_sample_num = len(pred_item_list)\n",
    "    print(\"Accuracy: \", correct_num / total_sample_num)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\", total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "def evaluate_text_generation_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table-to-text benchmark.\n",
    "    Metric: bleu.\n",
    "    More metrics like ROUGE or LLM-as-a-judge rating are needed for a more robust evaluation.\n",
    "    \"\"\"\n",
    "    bleu = BLEU()\n",
    "    output_text_list = [] # output text \n",
    "    reference_text_list = [] # reference text list\n",
    "    for item in pred_item_list:\n",
    "        pred_text = item['text']\n",
    "        item_id = item['question_id']\n",
    "        ori_item = item_id_to_test_item[item_id]\n",
    "        gold_text = ori_item['output']\n",
    "        assert gold_text not in ['','None']\n",
    "        output_text_list.append(pred_text)\n",
    "        reference_text_list.append(gold_text)\n",
    "    assert len(output_text_list) == len(reference_text_list)\n",
    "    bleu_score = bleu.corpus_score(output_text_list, [reference_text_list])\n",
    "    print(\"Benchmark: \",benchmark_name)\n",
    "    print(\"BLEU score:\",bleu_score)\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the predicted data\n",
    "benchmark_name_to_predicted_item_list = read_llava_prediction_file(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_name_list = ['TABMWP','WTQ','HiTab','TAT-QA','TabFact','InfoTabs','FeTaQA']\n",
    "\n",
    "\n",
    "for benchmark_name in benchmark_name_list:\n",
    "    predicted_item_list = benchmark_name_to_predicted_item_list[benchmark_name]\n",
    "    if benchmark_name in [\"TABMWP\", \"WTQ\", \"TAT-QA\", \"TabFact\", \"InfoTabs\"]:\n",
    "        evaluate_tqa_questions(benchmark_name,predicted_item_list)\n",
    "    if benchmark_name in [\"HiTab\"]:\n",
    "        evaluate_tqa_questions_hitab(benchmark_name,predicted_item_list)\n",
    "    elif benchmark_name in [\"FeTaQA\"]:\n",
    "        evaluate_text_generation_questions(benchmark_name,predicted_item_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
